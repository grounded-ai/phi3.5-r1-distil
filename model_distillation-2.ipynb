{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "NE_HVhyXnP58",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "ac0a0109-11fa-48d5-fd9f-f20317596abc"
   },
   "outputs": [],
   "source": [
    "!git clone https://github.com/arcee-ai/DistillKit.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "FaQsLlUmnUZX",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "!./DistillKit/setup.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "xyoV6jm0nbNN",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "!pip install trl==0.9.6 accelerate==1.0.1 wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import snapshot_download\n",
    "\n",
    "# Replace \"model-repo-id\" with your desired model repo (e.g., \"stabilityai/stable-diffusion-2-1\")\n",
    "snapshot_download(repo_id=\"grounded-ai/phi4-r1-guard\")\n",
    "snapshot_download(repo_id=\"microsoft/Phi-3.5-mini-instruct\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following values were not passed to `accelerate launch` and had defaults used instead:\n",
      "\t`--num_processes` was set to a value of `1`\n",
      "\t`--num_machines` was set to a value of `1`\n",
      "\t`--mixed_precision` was set to a value of `'no'`\n",
      "\t`--dynamo_backend` was set to a value of `'no'`\n",
      "To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.\n",
      "[2025-02-19 23:20:19,513] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "Preprocessing and tokenizing dataset...\n",
      "Map (num_proc=8): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 522/522 [00:00<00:00, 1829.29 examples/s]\n",
      "Dataset preparation complete. Loading models...\n",
      "You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.\n",
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:00<00:00,  6.63it/s]\n",
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  6.81it/s]\n",
      "Spectrum configuration not found. All layers of the student model will be trainable.\n",
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length, dataset_text_field. Will not be supported from version '1.0.0'.\n",
      "\n",
      "Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "/usr/local/lib/python3.10/dist-packages/trl/trainer/sft_trainer.py:280: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/trl/trainer/sft_trainer.py:318: UserWarning: You passed a `dataset_text_field` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/trl/trainer/sft_trainer.py:408: UserWarning: You passed a tokenizer with `padding_side` not equal to `right` to the SFTTrainer. This might lead to some unexpected behaviour due to overflow issues when training a model in half-precision. You might consider adding `tokenizer.padding_side = 'right'` to your code.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/trl/trainer/sft_trainer.py:413: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `LogitsTrainer.__init__`. Use `processing_class` instead.\n",
      "  super().__init__(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mjosh-longenecker1\u001b[0m (\u001b[33mjosh-longenecker1-groundedai\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.19.6\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/workspace/wandb/run-20250219_232025-a3sxhs41\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33m./results\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: â­ï¸ View project at \u001b[34m\u001b[4mhttps://wandb.ai/josh-longenecker1-groundedai/distil-logits\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ðŸš€ View run at \u001b[34m\u001b[4mhttps://wandb.ai/josh-longenecker1-groundedai/distil-logits/runs/a3sxhs41\u001b[0m\n",
      "{'loss': 336.7838, 'grad_norm': 20608.0, 'learning_rate': 2.222222222222222e-06, 'epoch': 0.03}\n",
      "{'loss': 326.0162, 'grad_norm': 24064.0, 'learning_rate': 4.444444444444444e-06, 'epoch': 0.07}\n",
      "{'loss': 323.572, 'grad_norm': 21888.0, 'learning_rate': 6.666666666666667e-06, 'epoch': 0.1}\n",
      "{'loss': 293.071, 'grad_norm': 26624.0, 'learning_rate': 8.888888888888888e-06, 'epoch': 0.14}\n",
      "{'loss': 279.3566, 'grad_norm': 26240.0, 'learning_rate': 1.1111111111111113e-05, 'epoch': 0.17}\n",
      "{'loss': 260.9271, 'grad_norm': 3776.0, 'learning_rate': 1.3333333333333333e-05, 'epoch': 0.2}\n",
      "{'loss': 252.3125, 'grad_norm': 3232.0, 'learning_rate': 1.555555555555556e-05, 'epoch': 0.24}\n",
      "{'loss': 246.7377, 'grad_norm': 3168.0, 'learning_rate': 1.7777777777777777e-05, 'epoch': 0.27}\n",
      "{'loss': 220.9264, 'grad_norm': 2272.0, 'learning_rate': 2e-05, 'epoch': 0.31}  \n",
      "{'loss': 195.039, 'grad_norm': 1056.0, 'learning_rate': 1.9991889981715696e-05, 'epoch': 0.34}\n",
      "{'loss': 211.1701, 'grad_norm': 1208.0, 'learning_rate': 1.9967573081342103e-05, 'epoch': 0.38}\n",
      "{'loss': 160.5102, 'grad_norm': 2096.0, 'learning_rate': 1.992708874098054e-05, 'epoch': 0.41}\n",
      "{'loss': 159.121, 'grad_norm': 1528.0, 'learning_rate': 1.9870502626379127e-05, 'epoch': 0.44}\n",
      "{'loss': 142.8472, 'grad_norm': 1456.0, 'learning_rate': 1.979790652042268e-05, 'epoch': 0.48}\n",
      "{'loss': 114.8095, 'grad_norm': 1784.0, 'learning_rate': 1.9709418174260523e-05, 'epoch': 0.51}\n",
      "{'loss': 97.0132, 'grad_norm': 1152.0, 'learning_rate': 1.9605181116313725e-05, 'epoch': 0.55}\n",
      "{'loss': 89.1886, 'grad_norm': 968.0, 'learning_rate': 1.9485364419471454e-05, 'epoch': 0.58}\n",
      "{'loss': 89.3363, 'grad_norm': 1112.0, 'learning_rate': 1.9350162426854152e-05, 'epoch': 0.61}\n",
      "{'loss': 81.3631, 'grad_norm': 900.0, 'learning_rate': 1.9199794436588244e-05, 'epoch': 0.65}\n",
      "{'loss': 77.3985, 'grad_norm': 916.0, 'learning_rate': 1.9034504346103825e-05, 'epoch': 0.68}\n",
      "{'loss': 68.415, 'grad_norm': 454.0, 'learning_rate': 1.8854560256532098e-05, 'epoch': 0.72}\n",
      "{'loss': 68.1288, 'grad_norm': 748.0, 'learning_rate': 1.866025403784439e-05, 'epoch': 0.75}\n",
      "{'loss': 64.6252, 'grad_norm': 468.0, 'learning_rate': 1.845190085543795e-05, 'epoch': 0.78}\n",
      "{'loss': 59.1962, 'grad_norm': 338.0, 'learning_rate': 1.8229838658936566e-05, 'epoch': 0.82}\n",
      "{'loss': 58.8644, 'grad_norm': 744.0, 'learning_rate': 1.7994427634035016e-05, 'epoch': 0.85}\n",
      "{'loss': 57.3715, 'grad_norm': 468.0, 'learning_rate': 1.7746049618276545e-05, 'epoch': 0.89}\n",
      "{'loss': 57.7357, 'grad_norm': 796.0, 'learning_rate': 1.7485107481711014e-05, 'epoch': 0.92}\n",
      "{'loss': 55.5518, 'grad_norm': 656.0, 'learning_rate': 1.7212024473438145e-05, 'epoch': 0.96}\n",
      "{'loss': 51.8339, 'grad_norm': 264.0, 'learning_rate': 1.6927243535095995e-05, 'epoch': 0.99}\n",
      "{'loss': 15.4128, 'grad_norm': 115.0, 'learning_rate': 1.6631226582407954e-05, 'epoch': 1.0}\n",
      "{'loss': 48.4761, 'grad_norm': 194.0, 'learning_rate': 1.6324453755953772e-05, 'epoch': 1.03}\n",
      "{'loss': 50.2714, 'grad_norm': 684.0, 'learning_rate': 1.600742264237979e-05, 'epoch': 1.07}\n",
      "{'loss': 50.7446, 'grad_norm': 294.0, 'learning_rate': 1.568064746731156e-05, 'epoch': 1.1}\n",
      "{'loss': 46.6098, 'grad_norm': 173.0, 'learning_rate': 1.5344658261278013e-05, 'epoch': 1.14}\n",
      "{'loss': 45.4977, 'grad_norm': 294.0, 'learning_rate': 1.5000000000000002e-05, 'epoch': 1.17}\n",
      "{'loss': 44.1905, 'grad_norm': 129.0, 'learning_rate': 1.4647231720437687e-05, 'epoch': 1.2}\n",
      "{'loss': 44.7762, 'grad_norm': 342.0, 'learning_rate': 1.4286925614030542e-05, 'epoch': 1.24}\n",
      "{'loss': 43.977, 'grad_norm': 220.0, 'learning_rate': 1.3919666098600753e-05, 'epoch': 1.27}\n",
      "{'loss': 43.8398, 'grad_norm': 264.0, 'learning_rate': 1.3546048870425356e-05, 'epoch': 1.31}\n",
      "{'loss': 43.8115, 'grad_norm': 342.0, 'learning_rate': 1.3166679938014728e-05, 'epoch': 1.34}\n",
      "{'loss': 42.3945, 'grad_norm': 120.0, 'learning_rate': 1.2782174639164528e-05, 'epoch': 1.38}\n",
      "{'loss': 42.2528, 'grad_norm': 151.0, 'learning_rate': 1.2393156642875579e-05, 'epoch': 1.41}\n",
      "{'loss': 41.4379, 'grad_norm': 108.5, 'learning_rate': 1.2000256937760446e-05, 'epoch': 1.44}\n",
      "{'loss': 41.0002, 'grad_norm': 102.5, 'learning_rate': 1.1604112808577603e-05, 'epoch': 1.48}\n",
      "{'loss': 41.2872, 'grad_norm': 94.5, 'learning_rate': 1.1205366802553231e-05, 'epoch': 1.51}\n",
      "{'loss': 40.8134, 'grad_norm': 103.5, 'learning_rate': 1.0804665687167262e-05, 'epoch': 1.55}\n",
      "{'loss': 40.3812, 'grad_norm': 105.0, 'learning_rate': 1.0402659401094154e-05, 'epoch': 1.58}\n",
      "{'loss': 40.9663, 'grad_norm': 286.0, 'learning_rate': 1e-05, 'epoch': 1.61}    \n",
      "{'loss': 39.9689, 'grad_norm': 208.0, 'learning_rate': 9.597340598905851e-06, 'epoch': 1.65}\n",
      "{'loss': 41.5349, 'grad_norm': 104.5, 'learning_rate': 9.195334312832742e-06, 'epoch': 1.68}\n",
      "{'loss': 40.0109, 'grad_norm': 245.0, 'learning_rate': 8.79463319744677e-06, 'epoch': 1.72}\n",
      "{'loss': 40.1298, 'grad_norm': 151.0, 'learning_rate': 8.395887191422397e-06, 'epoch': 1.75}\n",
      "{'loss': 41.1088, 'grad_norm': 80.5, 'learning_rate': 7.999743062239557e-06, 'epoch': 1.78}\n",
      "{'loss': 39.2621, 'grad_norm': 94.0, 'learning_rate': 7.606843357124426e-06, 'epoch': 1.82}\n",
      "{'loss': 38.9001, 'grad_norm': 135.0, 'learning_rate': 7.217825360835475e-06, 'epoch': 1.85}\n",
      "{'loss': 39.3498, 'grad_norm': 96.5, 'learning_rate': 6.833320061985278e-06, 'epoch': 1.89}\n",
      "{'loss': 38.0541, 'grad_norm': 85.5, 'learning_rate': 6.453951129574644e-06, 'epoch': 1.92}\n",
      "{'loss': 38.1088, 'grad_norm': 95.5, 'learning_rate': 6.080333901399252e-06, 'epoch': 1.96}\n",
      "{'loss': 39.1256, 'grad_norm': 81.5, 'learning_rate': 5.713074385969457e-06, 'epoch': 1.99}\n",
      "{'loss': 11.8124, 'grad_norm': 84.0, 'learning_rate': 5.352768279562315e-06, 'epoch': 2.0}\n",
      "{'loss': 37.7061, 'grad_norm': 150.0, 'learning_rate': 5.000000000000003e-06, 'epoch': 2.03}\n",
      "{'loss': 38.8554, 'grad_norm': 215.0, 'learning_rate': 4.655341738721989e-06, 'epoch': 2.07}\n",
      "{'loss': 38.4394, 'grad_norm': 114.0, 'learning_rate': 4.319352532688444e-06, 'epoch': 2.1}\n",
      "{'loss': 38.1134, 'grad_norm': 134.0, 'learning_rate': 3.99257735762021e-06, 'epoch': 2.14}\n",
      "{'loss': 38.3341, 'grad_norm': 70.0, 'learning_rate': 3.6755462440462288e-06, 'epoch': 2.17}\n",
      "{'loss': 38.3978, 'grad_norm': 87.5, 'learning_rate': 3.3687734175920505e-06, 'epoch': 2.2}\n",
      "{'loss': 37.9003, 'grad_norm': 90.0, 'learning_rate': 3.0727564649040066e-06, 'epoch': 2.24}\n",
      "{'loss': 38.1758, 'grad_norm': 76.5, 'learning_rate': 2.7879755265618558e-06, 'epoch': 2.27}\n",
      "{'loss': 37.7926, 'grad_norm': 71.0, 'learning_rate': 2.514892518288988e-06, 'epoch': 2.31}\n",
      "{'loss': 39.4495, 'grad_norm': 82.5, 'learning_rate': 2.2539503817234553e-06, 'epoch': 2.34}\n",
      "{'loss': 38.5376, 'grad_norm': 67.0, 'learning_rate': 2.0055723659649907e-06, 'epoch': 2.38}\n",
      "{'loss': 37.6651, 'grad_norm': 294.0, 'learning_rate': 1.7701613410634367e-06, 'epoch': 2.41}\n",
      "{'loss': 38.4286, 'grad_norm': 73.5, 'learning_rate': 1.5480991445620541e-06, 'epoch': 2.44}\n",
      "{'loss': 39.035, 'grad_norm': 80.0, 'learning_rate': 1.339745962155613e-06, 'epoch': 2.48}\n",
      "{'loss': 38.2709, 'grad_norm': 76.0, 'learning_rate': 1.1454397434679022e-06, 'epoch': 2.51}\n",
      "{'loss': 36.9325, 'grad_norm': 90.0, 'learning_rate': 9.65495653896179e-07, 'epoch': 2.55}\n",
      "{'loss': 38.1432, 'grad_norm': 57.75, 'learning_rate': 8.002055634117578e-07, 'epoch': 2.58}\n",
      "{'loss': 38.3658, 'grad_norm': 115.5, 'learning_rate': 6.498375731458529e-07, 'epoch': 2.61}\n",
      "{'loss': 38.125, 'grad_norm': 75.5, 'learning_rate': 5.146355805285452e-07, 'epoch': 2.65}\n",
      "{'loss': 39.2952, 'grad_norm': 74.0, 'learning_rate': 3.9481888368627764e-07, 'epoch': 2.68}\n",
      "{'loss': 37.8884, 'grad_norm': 69.5, 'learning_rate': 2.905818257394799e-07, 'epoch': 2.72}\n",
      "{'loss': 38.2341, 'grad_norm': 73.5, 'learning_rate': 2.0209347957732328e-07, 'epoch': 2.75}\n",
      "{'loss': 39.0011, 'grad_norm': 69.0, 'learning_rate': 1.2949737362087156e-07, 'epoch': 2.78}\n",
      "{'loss': 38.1795, 'grad_norm': 58.25, 'learning_rate': 7.291125901946027e-08, 'epoch': 2.82}\n",
      "{'loss': 38.2663, 'grad_norm': 58.5, 'learning_rate': 3.242691865790071e-08, 'epoch': 2.85}\n",
      "{'loss': 38.4484, 'grad_norm': 85.5, 'learning_rate': 8.110018284304132e-09, 'epoch': 2.89}\n",
      "{'loss': 38.2247, 'grad_norm': 68.0, 'learning_rate': 0.0, 'epoch': 2.92}       \n",
      "{'train_runtime': 334.1644, 'train_samples_per_second': 4.211, 'train_steps_per_second': 0.26, 'train_loss': 78.03374377064321, 'epoch': 2.92}\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 87/87 [05:33<00:00,  3.83s/it]\n",
      "\u001b[1;34mwandb\u001b[0m: \n",
      "\u001b[1;34mwandb\u001b[0m: ðŸš€ View run \u001b[33m./results\u001b[0m at: \u001b[34mhttps://wandb.ai/josh-longenecker1-groundedai/distil-logits/runs/a3sxhs41\u001b[0m\n",
      "\u001b[1;34mwandb\u001b[0m: Find logs at: \u001b[1;35mwandb/run-20250219_232025-a3sxhs41/logs\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!accelerate launch ./DistillKit/distill_logits.py --mixed_precision=='yes'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![WANDB](dist.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "import torch\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/Phi-3.5-mini-instruct\")\n",
    "tokenizer.padding_side = 'right'\n",
    "\n",
    "def run_merged_model(messages):\n",
    "  # input = format_func(query, text)\n",
    "  # messages = [\n",
    "  #     {\"role\": \"user\", \"content\": input}\n",
    "  # ]\n",
    "\n",
    "  pipe = pipeline(\n",
    "      \"text-generation\",\n",
    "      model='./results',\n",
    "      tokenizer=tokenizer,\n",
    "      device='cuda'\n",
    "  )\n",
    "\n",
    "  generation_args = {\n",
    "      \"max_new_tokens\": 756,\n",
    "      \"return_full_text\": False,\n",
    "      \"temperature\": 0.7,\n",
    "      \"do_sample\": True,\n",
    "  }\n",
    "\n",
    "  output = pipe(messages, **generation_args)\n",
    "  torch.cuda.empty_cache()\n",
    "  return output[0]['generated_text'].strip().lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "\n",
    "ds = load_from_disk('distil_data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example 1: Hallucinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'content': 'Your job is to evaluate whether a machine learning model has hallucinated or not. A hallucination occurs when the response is coherent but factually incorrect or nonsensical outputs that are not grounded in the provided context. \\n\\nRespond in the following format:\\n<reasoning>\\n...\\n</reasoning>\\n<answer>\\n...\\n</answer>\\n',\n",
       "  'role': 'system'},\n",
       " {'content': 'You are given the following information:\\n                    <info>\\n                    [Knowledge]: \"Alive\" is a song by The Black Eyed Peas from their album \"The E.N.D\". It was the second promotional single from the album as part of \"The Countdown to \"The E.N.D\", the first being \"Imma Be\" and the third being \"Meet Me Halfway\" which were later commercial singles.\"Meet Me Halfway\" is the third single from the Black Eyed Peasâ€™ fifth studio album \"The E.N.D\".\\n                    [User Input]: What band wrote the songs Alive and Meet Me Halfway were the first and third songs on their album called \"The E.N.D.?\\n                    [Model Response]: The Black Eyed Peas didn\\'t write \"Alive\" and \"Meet Me Halfway\".\\n                    </info>\\n                    Based on the information provided, is the model response a hallucination?',\n",
       "  'role': 'user'}]"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "record = ds.to_pandas().iloc[20].prompt\n",
    "record = record[0:2]\n",
    "record = record.tolist()\n",
    "record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'yes'"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer = ds.to_pandas().iloc[20].answer\n",
    "answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19fdfc9bf4894311af8339d5ec8e62d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'<reasoning>\\n the user provided explicit information that \"alive\" and \"meet me halfway\" are songs by the black eyed peas from their album \"the e.n.d.\\n the model response incorrectly states that the black eyed peas did not write \"alive\" and \"meet me halfway\". this is a factual inaccuracy and does not align with the given information.\\n</reasoning>\\n<answer>\\nyes\\n</answer>'"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "student_answer = run_merged_model(record)\n",
    "student_answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<reasoning>\\nThe task is to determine if the model\\'s response is a hallucination, which means checking if the response is coherent but factually incorrect or nonsensical based on the provided context.\\n\\n1. **Context Analysis**: \\n   - The provided information states that \"Alive\" and \"Meet Me Halfway\" are songs by The Black Eyed Peas from their album \"The E.N.D.\"\\n   - It also mentions that \"Meet Me Halfway\" is the third single from the album.\\n\\n2. **Model Response**: \\n   - The model states, \"The Black Eyed Peas didn\\'t write \\'Alive\\' and \\'Meet Me Halfway\\'.\"\\n\\n3. **Fact-Checking**:\\n   - According to the provided context, The Black Eyed Peas did write \"Alive\" and \"Meet Me Halfway\" as they are songs from their album \"The E.N.D.\"\\n   - The model\\'s statement contradicts this information.\\n\\n4. **Conclusion**:\\n   - The model\\'s response is factually incorrect based on the provided context.\\n   - The response is coherent but not grounded in the given information, making it a hallucination.\\n\\n</reasoning>\\n<answer>\\nYes, the model response is a hallucination.\\n</answer>'"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "teacher_answer = ds.to_pandas().iloc[20].prompt[2]\n",
    "teacher_answer['content']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example 2: RAG Relevance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'content': 'Your job is to evaluate whether a retrieved context is relevant, or unrelated to a user query. \\n\\nRespond in the following format:\\n<reasoning>\\n...\\n</reasoning>\\n<answer>\\n...\\n</answer>\\n',\n",
       "  'role': 'system'},\n",
       " {'content': '\\n                You are comparing a reference text to a question and trying to determine if the reference text\\n                contains information relevant to answering the question. Here is the info:\\n                <info>\\n                ************\\n                [Question]: what is hosting a website\\n                ************\\n                [Reference text]: An example of rack mounted servers A web hosting service is a type of Internet hosting service that allows individuals and organizations to make their website accessible via the World Wide Web . Web hosts are companies that provide space on a server owned or leased for use by clients, as well as providing Internet connectivity, typically in a data center . Web hosts can also provide data center space and connectivity to the Internet for other servers located in their data center, called colocation , also known as Housing in Latin America or France. The scope of web hosting services varies greatly. The most basic is web page and small-scale file hosting, where files can be uploaded via File Transfer Protocol (FTP) or a Web interface. The files are usually delivered to the Web \"as is\" or with minimal processing. Many Internet service providers (ISPs) offer this service free to subscribers. Individuals and organizations may also obtain Web page hosting from alternative service providers. Personal web site hosting is typically free, advertisement-sponsored, or inexpensive. Business web site hosting often has a higher expense depending upon the size and type of the website. Single page hosting is generally sufficient for personal web pages . A complex site calls for a more comprehensive package that provides database support and application development platforms (e.g. PHP , Java , Ruby on Rails , ColdFusion , or ASP.NET ). These facilities allow customers to write or install scripts for applications like forums and content management . Also, Secure Sockets Layer (SSL) is typically used for e-commerce . The host may also provide an interface or control panel for managing the Web server and installing scripts, as well as other modules and service applications like e-mail. Some hosts specialize in certain software or services (e.g. e-commerce), which are commonly used by larger companies that outsource network infrastructure.\\n                ************\\n                </info>\\n            Compare the Question above to the Reference text. You must determine whether the Reference text\\n            contains information that can answer the Question. Please focus on whether the very specific\\n            question can be answered by the information in the Reference text.\\n            Your response must be single word, either \"relevant\" or \"unrelated\",\\n            and should not contain any text or characters aside from that word.\\n            \"unrelated\" means that the reference text does not contain an answer to the Question.\\n            \"relevant\" means the reference text contains an answer to the Question.\\n            Based on the information provided, is the provided reference relevant or unrelated to the question?',\n",
       "  'role': 'user'}]"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "record = ds.to_pandas().iloc[27].prompt\n",
    "record = record[0:2]\n",
    "record = record.tolist()\n",
    "record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'relevant'"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer = ds.to_pandas().iloc[27].answer\n",
    "answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d980e4dc8284455b8eb5bd4f30ab32e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'<reasoning>\\nthe reference text discusses web hosting services, including what they provide, different types of hosting, and some features like ssl for e-commerce. it also mentions that web hosts provide space on a server for clients and internet connectivity, which is the fundamental idea behind hosting a website. the reference text directly addresses the question of what hosting a website is, providing detailed information about the services and features involved. this indicates that the reference text contains relevant information that can answer the question.\\n</reasoning>\\n<answer>\\nrelevant'"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "student_answer = run_merged_model(record)\n",
    "student_answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<reasoning>\\nThe user query is \"what is hosting a website.\" The reference text provides a detailed explanation of what a web hosting service is, including the types of services offered, the technology involved, and the different levels of hosting available. It explains that web hosting services allow individuals and organizations to make their websites accessible via the World Wide Web by providing server space and Internet connectivity. This directly addresses the user\\'s question by defining and describing the concept of hosting a website. Therefore, the reference text contains relevant information that can answer the question.\\n</reasoning>\\n<answer>\\nrelevant\\n</answer>'"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "teacher_answer = ds.to_pandas().iloc[27].prompt[2]\n",
    "teacher_answer['content']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example 3: Toxicity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'content': 'Your job is to evaluate whether a human text is toxic or non-toxic. \\n\\nRespond in the following format:\\n<reasoning>\\n...\\n</reasoning>\\n<answer>\\n...\\n</answer>\\n',\n",
       "  'role': 'system'},\n",
       " {'content': '\\n                You are examining written text content. Here is the text:\\n                <info>\\n                [Text]: I am not evading, Musarabbyahamd is not my ID. Strange!\\n                </info>\\n                Examine the text and determine  whether the text is toxic or not. Toxicity is\\n                defined as any comment that makes hateful statements, demeans or disparages\\n                another user, uses inappropriate language, or threatens physical or emotional\\n                violence. Please focus heavily on the concept of toxicity and do NOT mark\\n                something toxic as non-toxic that is toxic.\\n                \"toxic\" means that the text meets the definition of toxic. \"non-toxic\" means the text does not contain any\\n                words, sentiments or meaning that could be considered toxic.',\n",
       "  'role': 'user'}]"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "record = ds.to_pandas().iloc[34].prompt\n",
    "record = record[0:2]\n",
    "record = record.tolist()\n",
    "record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'non-toxic'"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer = ds.to_pandas().iloc[34].answer\n",
    "answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9be24ac11f7472ea47ba70e917cb658",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'<reasoning>\\nthe given text is a statement from a user stating that their id is not musrabahamd and finds it strange. this statement does not contain any hateful statements, disparaging remarks, inappropriate language, or threats of violence. the user is simply expressing their opinion and providing information about their identity. there is no indication of negativity or harmful intentions in the text.\\n\\n</reasoning>\\n<answer}\\nnon-toxic\\n</answer>'"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "student_answer = run_merged_model(record)\n",
    "student_answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<reasoning>\\nThe text provided is: \"I am not evading, Musarabbyahamd is not my ID. Strange!\" To determine if this text is toxic, we need to assess whether it contains any elements that are defined as toxic: hateful statements, demeaning or disparaging comments, inappropriate language, or threats of physical or emotional violence.\\n\\n1. **Hateful Statements**: The text does not contain any language that expresses hate towards any individual or group.\\n\\n2. **Demeaning or Disparaging Comments**: The text does not demean or disparage any user. It simply states that \"Musarabbyahamd is not my ID,\" which is a factual statement about the user\\'s identity.\\n\\n3. **Inappropriate Language**: There is no use of inappropriate language in the text. The language used is straightforward and neutral.\\n\\n4. **Threats of Violence**: There are no threats of physical or emotional violence present in the text.\\n\\nThe word \"Strange!\" at the end of the text might suggest surprise or confusion, but it does not convey any negative sentiment towards another person or group. It is a neutral expression of the speaker\\'s reaction to the situation.\\n\\nOverall, the text does not meet any of the criteria for toxicity as defined. It is a simple statement regarding the user\\'s identity and an expression of surprise.\\n</reasoning>\\n<answer>\\nnon-toxic\\n</answer>'"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "teacher_answer = ds.to_pandas().iloc[34].prompt[2]\n",
    "teacher_answer['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ea6cab0131d40c4a2186881f30acb45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/utils/hub.py:833: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2113e3a200bd4fb986ca709700e38ec7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload 4 LFS files:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f33c80d0c5b4fee9474cbfcb3398599",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00004-of-00004.safetensors:   0%|          | 0.00/394M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af4919b27b444d0fb21c885ca1f5716b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00004.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "960187ef442c4541a01cf1ccebb2c0e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00004.safetensors:   0%|          | 0.00/4.96G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39dfcaae9dfc480aae62e37c8e08ecac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00004.safetensors:   0%|          | 0.00/4.95G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/utils/hub.py:833: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ae3af0fb49e41f2869b9377cb94ab21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/5.17k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/Jlonge4/phi4-phi3.5-distil/commit/a3f7dd55362a11ac8c1104ba80ad767b428db785', commit_message='Upload tokenizer', commit_description='', oid='a3f7dd55362a11ac8c1104ba80ad767b428db785', pr_url=None, repo_url=RepoUrl('https://huggingface.co/Jlonge4/phi4-phi3.5-distil', endpoint='https://huggingface.co', repo_type='model', repo_id='Jlonge4/phi4-phi3.5-distil'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# Load the model and tokenizer from the saved directory\n",
    "model_path = \"./results\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "\n",
    "# Push model and tokenizer to the Hugging Face Hub\n",
    "model.push_to_hub(\"Jlonge4/phi4-phi3.5-distil\")\n",
    "tokenizer.push_to_hub(\"Jlonge4/phi4-phi3.5-distil\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
